\documentclass[11pt, a4paper]{article}
\usepackage{amsmath}
%\usepackage{bm}
%\usepackage{ellipsis}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
%
%% Typography
\usepackage{fontspec}
\setmainfont{CMU Concrete}
\usepackage{eulervm}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{siunitx}
%
%% Source code
% 
% \usepackage[medium, compact]{titlesec}
%%
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[top=2.82cm, bottom=2.82cm, left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Running BritLLM from scratch}
\author{James Geddes}
\date{\today}
%%
\newcommand{\setR}{\mathbold{R}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\isdef}{\mathrel{\stackrel{\text{def}}{=}}}
\hyphenation{anti-sym-met-ric}
%%
% \usepackage[backend=biber]{biblatex} 
%\addbibresource{BIBLIOGRAPHY-GOES-HERE.bib}
% \DefineBibliographyStrings{english}{
%   andothers = {\mkbibemph{et\addabbrvspace{al}\adddot}}
% }
%%
\begin{document}
\maketitle

\section{Deep learning}

A \emph{deep learning model} is a kind of map,
$M\colon \setR^{d_\text{in}} \to \setR^{d_\text{out}}$, of a particular
functional form, to be described below.

The interpretation of an element of the domain, $\setR^{d_\text{in}}$,
or the codomain, $\setR^{d_\text{out}}$, depends on the particular
model.

\emph{Example:} The job of a ``large language model'' is to
stochastically emit ``the next word'' given a sequence of words. The
output of the model, an element of $\setR^{d_\text{out}}$, is here
construed as a tuple, $(p_1, \dotsc, p_{d_\text{out}})$, where each
$p_i$ is the probability of emitting word number~$i$. That is, for
this particular example, the output is interpreted as a categorical
distribution over the set of possible output words, from which the
next word is chosen with the given probabilities.

Elements of the input and output spaces are typically called
``vectors,'' although in fact there is often no meaningful
interpretation of the canonical vector space structure on those
spaces. In the example above, the output space might instead more
reasonably be taken to be a \emph{subset} of $\setR^{d_\text{out}}$:
namely, the $(d_\text{out}-1)$-dimensional simplex.\footnote{Note that
  the (component-wise) sum of two elements of the simplex is not an
  element of the simplex. (Each $p_i$ must lie in the range $[0,1]$
  but the sum of two numbers between 0 and 1 is not necessarily a
  number between 0 and~1.)}

It is, however, useful to think of inputs and outputs (and, as it will
turn out, intermediate values) as \emph{arrays}: an array is an
ordered sequence of values, all of which are, in some sense, ``the
same kind of value.''

\section{BritLLM}

BritLLM is a ``pretrained model.'' This section describes how to run
inference for this particular model starting with the raw data
files. The input to the model is a string; the output from the model
is a probability distribution over the vocabulary of the model.

The first step is to convert the input string to a ``vector.'' And the
first step in that process is to convert the input string to an array
of numbers.

\subsection{Tokenisation}

The vocabulary of the model is neither individual letters nor whole
words but instead parts of words, known as ``tokens.''

Each token has an identifying number. ``Tokenisation'' is the process
of converting a passage of text into its parts of words, and thence to
a list of integers. Different approaches to tokenisation are in use,
the one that Llama-type models (like BritLLM) often use is called
``byte-pair encoding'' [BPE]. A pretrained model comes with a
pretrained tokeniser\sidenote{The definition of the tokeniser is in
  the file \texttt{tokenizer.json}, and specifically in the dictionary
  element labelled \texttt{"model"}. There is some additional
  configuration in the file \texttt{tokenizer\_config.json}.} defined
by
\begin{enumerate}
\item A set, $T$, of sequences of bytes; and
  \item A map, $T\to [0 .. N-1]$.
\end{enumerate}
Note that the input string is regarded as a sequence of bytes. The
input string is supposed to be UTF-8 encoded Unicode but it is the
bytes that are tokenised (and not, for example, the Unicode
codepoints).

The process of converting a sequence of bytes to a list of integers
for this particular model and its associated tokeniser is as follows:
\begin{enumerate}
\item Replace each space with the character U+2581, ``lower one eighth
  block.'' (That is to say, with the sequence of bytes \verb|0xe2|,
  \verb|0x96|, \verb|0x81|.)
\end{enumerate}



\appendix

\section{BritLLM details}

\subsection{The Tokenizer}

Many models use the \sf{transformers} library's tokenisation
functionality, specifically
\begin{verbatim}
transfomers.AutoTokenizer.from_pretrained
\end{verbatim}
which is in \sf{models/auto/tokenization\_auto.py}. Here is my best
understanding of what that library does.

First, I think \sf{Autotokenizer} reads the file
\sf{tokenizer\_{}config.json} and decides what kind of tokeniser to
instantiate. For BritLLM, that config file seems to specify a
\sf{LlamaTokenizer}. So then I guess we need to look in
\sf{models/llama/tokenization\_llama.py}.

Okay, actually I ran the code and asked it what type of object was
produced, and it turns out that it is a ``fast tokenizer,'' which as I
understand it means that a Rust library is used. Specifically, it's of
class
\sf{transformers.models.llama.tokenization\_llama\_fast.LlamaTokenizerFast}. 

The table in the margin shows some of the properties of this object:
\begin{margintable}
\begin{tabular}{ll}
  \toprule
  \verb|bos_token| & \verb|</s>| \\
  \verb|eos_token| & \verb|</s>| \\
  \verb|unk_token| & \verb|<unk>| \\
  \bottomrule
\end{tabular}
\end{margintable}

                      
                      
                       
                      
  
There are $32\,000$ tokens in this particular tokenizer (from $0$ to $31\,999$). 

\subsubsection{Tokenizer configuration}

This is the file \sf{tokenizer\_config.json} from BritLLM\@.
{\small
\begin{verbatim}
{
  "add_bos_token": false,
  "add_eos_token": false,
  "added_tokens_decoder": {
    "0": {
      "content": "<unk>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "2": {
      "content": "</s>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    }
  },
  "bos_token": "</s>",
  "clean_up_tokenization_spaces": false,
  "eos_token": "</s>",
  "legacy": true,
  "model_max_length": 1000000000000000019884624838656,
  "pad_token": null,
  "sp_model_kwargs": {},
  "spaces_between_special_tokens": false,
  "tokenizer_class": "LlamaTokenizer",
  "unk_token": "<unk>",
  "use_default_system_prompt": false
}
\end{verbatim}}

\end{document}

%% Code from Huggingface

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("britllm/britllm-3b-v0.1")
model = AutoModelForCausalLM.from_pretrained("britllm/britllm-3b-v0.1")


%% Energy for one query

GH200 is ~1 TFLOPS and ~1 kW

So one second is

(a) 10^12 FLOP and (b) 10^3 J

So that's 10^9 FLOP per J

1 billion parameters is 10^9. Say 3 FLOP per parameter per token. And
~100 tokens per query (except there's some clever stuff going on)

That's 300 J per query.

Or 10^-4 kWh





%%% Local Variables:
%%% mode: latex
%%% TeX-engine: luatex
%%% End:



