\documentclass[11pt, a4paper]{article}
\usepackage{amsmath}
%\usepackage{bm}
%\usepackage{ellipsis}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
%
%% Typography
\usepackage{fontspec}
\setmainfont{CMU Concrete}
\usepackage{eulervm}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{siunitx}
%
%% Source code
% 
% \usepackage[medium, compact]{titlesec}
%%
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[top=2.82cm, bottom=2.82cm, left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Running BritLLM from scratch}
\author{James Geddes}
\date{\today}
%%
\newcommand{\setR}{\mathbold{R}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\isdef}{\mathrel{\stackrel{\text{def}}{=}}}
\hyphenation{anti-sym-met-ric}
%%
\usepackage[backend=biber]{biblatex} 
\addbibresource{llms.bib}
% \DefineBibliographyStrings{english}{
%   andothers = {\mkbibemph{et\addabbrvspace{al}\adddot}}
% }
%%
\begin{document}
\maketitle

\section{Introduction}

This note is intended to be a reference guide to running a
Llama-based, pre-trained model from scratch. Currently, it is based
around
BritLLM.\sidenote{\url{https://huggingface.co/britllm/britllm-3b-v0.1}}

\section{Deep learning}
A \emph{deep learning model} is a kind of map,
$M\colon \setR^{d_\text{in}} \to \setR^{d_\text{out}}$, of a particular
functional form, to be described below.

The interpretation of an element of the domain, $\setR^{d_\text{in}}$,
or the codomain, $\setR^{d_\text{out}}$, depends on the particular
model.

\emph{Example:} The job of a ``large language model'' is to
stochastically emit ``the next word'' given a sequence of words. The
output of the model, an element of $\setR^{d_\text{out}}$, is here
construed as a tuple, $(p_1, \dotsc, p_{d_\text{out}})$, where each
$p_i$ is the probability of emitting word number~$i$. That is, for
this particular example, the output is interpreted as a categorical
distribution over the set of possible output words, from which the
next word is chosen with the given probabilities.

Elements of the input and output spaces are typically called
``vectors,'' although in fact there is often no meaningful
interpretation of the canonical vector space structure on those
spaces. In the example above, the output space might instead more
reasonably be taken to be a \emph{subset} of $\setR^{d_\text{out}}$:
namely, the $(d_\text{out}-1)$-dimensional simplex.\footnote{Note that
  the (component-wise) sum of two elements of the simplex is not an
  element of the simplex. (Each $p_i$ must lie in the range $[0,1]$
  but the sum of two numbers between 0 and 1 is not necessarily a
  number between 0 and~1.)}

It is, however, useful to think of inputs and outputs (and, as it will
turn out, intermediate values) as \emph{arrays}: an array is an
ordered sequence of values, all of which are, in some sense, ``the
same kind of value.''

\section{BritLLM}

BritLLM is a ``pretrained model.'' This section describes how to run
inference for this particular model starting with the raw data
files. The input to the model is a string; the output from the model
is a probability distribution over the vocabulary of the model.

\subsection{Tokenisation}

The vocabulary of the model---its input and output---is neither
individual letters nor whole words but instead parts of words, known
as ``tokens.'' The input to the model is a sequence of tokens and the
output is a probability distribution over the vocabulary, the set of
all tokens.

Each token is a sequence of characters together with a unique
identifying number which, by abuse of language, we may also refer to
as the token. ``Tokenisation'' is the process of converting a passage
of text into its parts of words, and thence to a list of identifying
numbers. Different approaches to tokenisation are in use: the one that
Llama-type models (like BritLLM) is called ``byte-pair encoding''
[BPE]. In fact, there are variations on a theme even within this
single approach and the name itself is historical and now something of
a misnomer.
\begin{marginfigure}
  \footnotesize
  \caption{The precise tokenisation process for BritLLM, insofar as I
    understand it.\label{fig:toksteps}}
  \begin{enumerate}
  \item From the pretrained model, one is given a \emph{vocabulary} (a
    set of tokens) and a \emph{merge list} (an ordered sequence of
    merge rules). A merge rule is a pair of tokens to match, and a
    token with which to replace the matching pair.
  \item First, the input text is converted to (or understood as) a
    sequence of Unicode codepoints.
  \item Then, all spaces in the input sequence are replaced by the
    Unicode character U+2581 (This character, ``lower one eighth
    block,'' looks like a thick underscore.)\label{tokstep:replace}
  \item Unless the sequence is empty, the Unicode character U+2581 is
    also prepended to the beginning of the sequence.
  \item Each character in the input sequence is converted to a token,
    by reference to the vocabulary. If there is no token for a
    particular Unicode character, that character is converted to its
    UTF-8 representation as bytes, each byte in that representation is
    converted to a token, and the resulting tokens inserted into the
    sequence in place of the character. (In the Llama tokeniser, it
    appears that each byte is guaranteed to have a token of the form
    ``\texttt{<0xAA>}''.)
  \item Then the merge rules are processed in order, from first to
    last.
  \item For each merge rule, all adjacent pairs of tokens in the
    sequence matching the merge rule are replaced by the replacement
    token in the merge rule.\label{tokstep:merge}
  \item Step \ref{tokstep:merge} is repeated for each merge rule until
    there are no more merge rules.
  \end{enumerate}
\end{marginfigure}

In byte-pair encoding the process of tokenisation happens roughly as
follows. The input text is written as a sequence of characters and
those characters are converted to ``atomic'' tokens. Then, following
some rule, pairs of adjacent tokens are repeatedly chosen and replaced
by (or ``merged'') into a single token. This process continues until
no pair of adjacent tokens can be merged according to the rule. The
resulting sequence consists of tokens that represent words or parts of
words.

The above process is only a rough description: one must also make
specific choices. One such choice is the meaning of ``character.'' In
the original BPE methodology, a character meant a byte; in the variant
used by BritLLM it means a Unicode code point. Since not every Unicode
code point is a token, there is an additional question of what do to
with Unicode characters that are not represented by tokens. And so on.

The precise details, insofar as I have been able to work them out, are
given in figure~\ref{fig:toksteps}. See \textcite{zouhar2023formal}
for a decent overview of~BPE\@.



\printbibliography


\appendix

\section{BritLLM details}

\subsection{The Tokenizer}

Many models use the \sf{transformers} library's tokenisation
functionality, specifically
\begin{verbatim}
transfomers.AutoTokenizer.from_pretrained
\end{verbatim}
which is in \sf{models/auto/tokenization\_auto.py}. Here is my best
understanding of what that library does.

First, I think \sf{Autotokenizer} reads the file
\sf{tokenizer\_{}config.json} and decides what kind of tokeniser to
instantiate. For BritLLM, that config file seems to specify a
\sf{LlamaTokenizer}. So then I guess we need to look in
\sf{models/llama/tokenization\_llama.py}.

Okay, actually I ran the code and asked it what type of object was
produced, and it turns out that it is a ``fast tokenizer,'' which as I
understand it means that a Rust library is used. Specifically, it's of
class
\sf{transformers.models.llama.tokenization\_llama\_fast.LlamaTokenizerFast}. 

The table in the margin shows some of the properties of this object:
\begin{margintable}
\begin{tabular}{ll}
  \toprule
  \verb|bos_token| & \verb|</s>| \\
  \verb|eos_token| & \verb|</s>| \\
  \verb|unk_token| & \verb|<unk>| \\
  \bottomrule
\end{tabular}
\end{margintable}

                      
                      
                       
                      
  
There are $32\,000$ tokens in this particular tokenizer (from $0$ to $31\,999$). 

\subsubsection{Tokenizer configuration}

This is the file \sf{tokenizer\_config.json} from BritLLM\@.
{\small
\begin{verbatim}
{
  "add_bos_token": false,
  "add_eos_token": false,
  "added_tokens_decoder": {
    "0": {
      "content": "<unk>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "2": {
      "content": "</s>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    }
  },
  "bos_token": "</s>",
  "clean_up_tokenization_spaces": false,
  "eos_token": "</s>",
  "legacy": true,
  "model_max_length": 1000000000000000019884624838656,
  "pad_token": null,
  "sp_model_kwargs": {},
  "spaces_between_special_tokens": false,
  "tokenizer_class": "LlamaTokenizer",
  "unk_token": "<unk>",
  "use_default_system_prompt": false
}
\end{verbatim}}

\end{document}

%% Code from Huggingface

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("britllm/britllm-3b-v0.1")
model = AutoModelForCausalLM.from_pretrained("britllm/britllm-3b-v0.1")


%% Energy for one query

GH200 is ~1 TFLOPS and ~1 kW

So one second is

(a) 10^12 FLOP and (b) 10^3 J

So that's 10^9 FLOP per J

1 billion parameters is 10^9. Say 3 FLOP per parameter per token. And
~100 tokens per query (except there's some clever stuff going on)

That's 300 J per query.

Or 10^-4 kWh





%%% Local Variables:
%%% mode: latex
%%% TeX-engine: luatex
%%% End:



